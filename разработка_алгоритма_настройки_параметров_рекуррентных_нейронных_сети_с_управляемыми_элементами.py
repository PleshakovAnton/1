# -*- coding: utf-8 -*-
"""Разработка_алгоритма_настройки_параметров_рекуррентных_нейронных_сети_с_управляемыми_элементами.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OWMfZ6nxMIMqJP4kvWUlp35jL1zhT_aD
"""

import yfinance as yf
import datetime
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, Model, regularizers
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score
from scipy.stats import spearmanr
import matplotlib.pyplot as plt

# Выбираем акции кампании google
ticker = "GOOGL"

# задание временного интервала цены акций
start_date = "2014-01-01"
end_date = "2024-04-01"

# Получение данных с помощью Yahoo Finance API
df = yf.download(ticker, start=start_date, end=end_date)
df.describe()

# Выбор нужных столбцов
column = ['Open', 'Adj Close', 'Volume']
df_new = df[column]
df_new

# Визуализация изменения цены акций на заданном интервале
startDate = datetime.datetime(2014, 1, 1)
endDate = datetime.datetime(2024, 4, 1)
google_data = yf.Ticker('GOOGL')
google_df = google_data.history(start=startDate, end=endDate)
google_df['Close'].plot(title="GOOGL stock")
plt.grid(True)

# Выбор нужных столбцов
column = ['Open', 'Adj Close', 'Volume']
df_new = df[column]

# Преобразование данных
scaler = MinMaxScaler()
df_scaled = scaler.fit_transform(df_new)

# Параметры модели
sequence_length = 10
n_features = 2
n_attention_heads = 3
l1_reg = 0.001
l2_reg = 0.001
momentum = 0.9

# Подготовка данных
def prepare_data(data):
    X, y = [], []
    for i in range(len(data) - sequence_length):
        X.append(data[i:i+sequence_length, :-1])
        y.append(data[i+sequence_length, -2])  # Изменение индекса для предсказания столбца 'Adj Close'
    return np.array(X), np.array(y)

X, y = prepare_data(df_scaled)

# Разделение данных на тренировочную и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Рекуррентная модель с механизмом внимания и GRU
inputs = layers.Input(shape=(sequence_length, n_features))
attention = layers.MultiHeadAttention(num_heads=n_attention_heads, key_dim=64)(inputs, inputs)
gru_out = layers.GRU(64, return_sequences=True, kernel_regularizer=regularizers.l1_l2(l1=l1_reg, l2=l2_reg))(attention)
out = layers.Flatten()(gru_out)
out = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=l1_reg, l2=l2_reg))(out)
out = layers.Dense(1, kernel_regularizer=regularizers.l1_l2(l1=l1_reg, l2=l2_reg))(out)
model = Model(inputs, out)

# Компиляция модели с оптимизатором SGD с моментом
optimizer = tf.keras.optimizers.SGD(momentum=momentum)
model.compile(optimizer=optimizer, loss='mse')

# Обучение модели на тренировочных данных с использованием обратного вызова History
history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.1)

# Получение истории обучения
history_dict = history.history

# Предсказание на тестовых данных
y_pred = model.predict(X_test)

# Оценка модели
loss = model.evaluate(X_test, y_test) #Средняя квадратическая ошибка
r_squared = r2_score(y_test, y_pred) #Коэффициент детерминации
mae = mean_absolute_error(y_test, y_pred) #Среднее абсолютное значение разности между предсказанными значениями модели и их соответствующими истинными значениями
mape = mean_absolute_percentage_error(y_test, y_pred) #Средняя абсолютная процентная ошибка между предсказанными и истинными значениями
spearman_corr, _ = spearmanr(y_test, y_pred) # Коэффициент Спирмена

print("Spearman's correlation coefficient:", spearman_corr)
print("R-squared:", r_squared)
print("MAE:", mae)
print("MAPE:", mape)
print("Test Loss:", loss)

# Проверка наличия ключей 'loss' и 'val_loss'
if 'loss' in history_dict and 'val_loss' in history_dict:
    # Графики потерь на обучающем и валидационном наборах
    plt.plot(history_dict['loss'], label='Training Loss')
    plt.plot(history_dict['val_loss'], label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()
else:
    print("Ошибка: Не удалось найти историю потерь в обучении модели.")

# Вычисление ошибок
errors = y_pred.flatten() - y_test

# Построение гистограммы распределения ошибок
plt.figure(figsize=(10, 6))
plt.hist(errors, bins=50, edgecolor='k')
plt.title('Распределение ошибок')
plt.xlabel('Ошибка')
plt.ylabel('Частота')
plt.grid(True)
plt.show()

# Сравнение предсказанных и действительных значений
plt.figure(figsize=(14, 7))
plt.plot(y_test[0:50], label='Actual', color='blue')
plt.plot(y_pred[0:50], label='Predicted', color='red')

plt.title('Actual vs Predicted Stock Prices')
plt.xlabel('count')
plt.ylabel('Scaled Price')
plt.legend()
plt.show()

# Рассчитываем корреляцию между фактическими и предсказанными значениями
correlation = np.corrcoef(y_test.flatten(), y_pred.flatten())[0, 1]

# Создаем график корреляции
plt.figure(figsize=(8, 6))
plt.scatter(y_test.flatten(), y_pred.flatten(), color='blue', alpha=0.5)
plt.title('Correlation Plot')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.grid(True)
plt.text(0.1, 0.9, f'Correlation: {correlation:.2f}', transform=plt.gca().transAxes)
plt.show()

# Получаем историю потерь
train_loss = history_dict['loss']
val_loss = history_dict['val_loss']
epochs = range(1, len(train_loss) + 1)

# Создаем график
plt.figure(figsize=(8, 6))
plt.plot(epochs, train_loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()